# ma-supplement
Repo contains supplement to my Master's thesis

## Modified MultiPL-E Benchmark

This dataset contains the rust tasks from the `MultiPL-E` benchmark. Some tasks have been modified, for example to fix naming convention errors. Some parts are taken from the `HumanEval-Pack`. The original dataset of `MultiPL-E` can be found [here](https://github.com/nuprl/MultiPL-E/blob/main/prompts/humaneval-rs-reworded.jsonl) and that of `HumanEval-Pack` can be found [here](https://huggingface.co/datasets/bigcode/humanevalpack).

## Created Rust Benchmark

This dataset contains 40 hand-crafted Rust problems. The dataset is in the `MultiPL-E` format. The tasks can be completed without any knowledge of algorithms. Most of the tasks are written to include functions from the Rust `std` crate. Additionally, it contains lists for each task with functions and operators which should be generated by the llm in the samples. The dataset is created to evaluate the llms on idiomatic rust.

## Cite

### HumanEval-Pack

```
@article{muennighoff2023octopack,
      title={OctoPack: Instruction Tuning Code Large Language Models}, 
      author={Niklas Muennighoff and Qian Liu and Armel Zebaze and Qinkai Zheng and Binyuan Hui and Terry Yue Zhuo and Swayam Singh and Xiangru Tang and Leandro von Werra and Shayne Longpre},
      journal={arXiv preprint arXiv:2308.07124},
      year={2023}
}
```